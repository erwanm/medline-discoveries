---
title: "Mining Impactful Discoveries 1: Detecting Surges"
author: "Erwan Moreau"
date: "November 2021"
output: html_document
---


```{r setup, include=FALSE}
library(knitr)
library(rmarkdown)
knitr::opts_chunk$set(echo = TRUE)
```


# medline-discoveries: detecting surges (main documentation)

## Overview

This is the main documentation for the [Medline Discoveries repository](https://github.com/erwanm/medline-discoveries), which is the companion code for the paper "Mining impactful discoveries from the biomedical literature" **TODO link**.

There are three parts in the documentation:
     
- The [data collection](https://erwanm.github.io/medline-discoveries/data-collection) process describes how the input data was created.
- The ["Detecting surges" main documentation](https://erwanm.github.io/medline-discoveries/1-detecting-surges.html) (this document) describes how to generate the output data (i.e. detecting surges) using the provided R implementation.
- The ["Analysis" part](https://erwanm.github.io/medline-discoveries/2-analysis.html) proposes some additional analysis of the results, including how to generate the graphs and tables found in the paper.  



### Rmd Options

This document was generated from an [R Markdown](https://rmarkdown.rstudio.com/) source file. The source file is provided in the repository, allowing full reproducibility of the experiments presented in the paper. It can be executed through the RStudio interface ("knit" button) or as follows:

```
rmarkdown::render('1-detecting-surges.Rmd')
```

The `.Rmd` source document can be configured by modifying the following lines:

```{r myoptions}
# input directory
dataPath <- 'data/input'
# output dir
outputDir='data/output'
# filenames suffix (see 'input data format' below)
data.suffix <- '.ND.min100'
# set to TRUE for quick execution with a random subset of the data
shortVersion <- TRUE
```


## Requirements

### Software

This code requires R and a few R libraries:

- `data.table`
- `ggplot2`
- `plyr`
- `cowplot`
- `scales`
- `knitr` and `rmarkdown` (only needed for executing the Rmd source files)

### Data

The input data used for the experiment described in the paper can be obtained at **TODO link**. The process used to collect this data is described in the [data collection part](https://erwanm.github.io/medline-discoveries/data-collection).

By default the input data is expected in the subdirectory `data/input`.

### Input data format

The input data format is purposefully simple in order to permit the use of the software with different sources of data. There are two "variants" of the data:

* The **dynamic** data contains the frequency value for every years
* The **static** data contains the cumulated frequency across all years. 
    * By default located in a subdirectory `static`.
    
Both are made of two files, one for the individual frequency by concept and the other for the joint frequency by pair of concepts. The format for each file is as follows:

* `indiv.<suffix>`: `<year> <concept> <frequency> <multi frequency>`
    * The last column `<multi frequency>` is ignored.
* `joint.<suffix>`: `<year> <concept1> <concept2> <frequency>`
* `static/indiv.<suffix>`: `<concept> <frequency> [multi frequency] <term> <group>`
    * The last two columns are required for displaying the name of the concept/relation or filtering by semantic group. 
* `static/joint.<suffix>`: `<concept1> <concept2> <frequency>`

Additionally the two corresponding "totals" files are needed:

* `indiv.<suffix>.total`: `<year> <total unique concepts> <total documents> <total concepts occurrences>` 
* `static/indiv.<suffix>.total`: `<total documents> <total concepts occurrences>` 

In the joint files, it is expected that every pair of concepts (A,B) appears only once in the data for a given year (i.e. not "A B" and also "B A"). This condition can be satisfied by ordering every pair of concepts alphabetically: A < B.



# Main process

*Note: the full process is described in detail below. Readers who are only interested in the simplest way to run the process can go directly to the last part ["Processing multiple parameters and saving surges data"](#processing-multiple-parameters-and-saving-surges-data).*

## Initialization

The code can be loaded in the R interpreter with:

```{r sourcing}
source('discoveries.R')
```


## Loading the input data


```{r load dynamic}
dynamic_joint <- loadDynamicData(dir=dataPath,suffix=data.suffix, indivOrJoint = 'joint')
dynamic_indiv <- loadDynamicData(dir=dataPath,suffix=data.suffix, indivOrJoint = 'indiv')
dynamic_total <- loadDynamicTotalFile(dir=dataPath)
```

```{r, init.data.short, eval=shortVersion}
dynamic_joint<-pickRandomDynamic(dynamic_joint,n=1000)
```



### Statistics

* Number of concepts:

```{r}
nrow(unique(dynamic_indiv,by=key(dynamic_indiv)))
```

* Number of relations:

```{r}
nrow(unique(dynamic_joint,by=key(dynamic_joint)))
```

* Number of rows, i.e. pairs (relation,year):

```{r nrow1}
nrow(dynamic_joint)
```



## Preprocessing

This step fills the gap years with 0 frequency values and calculates the moving average if needed.

- This step is necessary even if the moving average is not used (default window of size 1).

Example with a moving average over a window of size 5:

```{r ma5}
relations.ma <- computeMovingAverage(dynamic_joint,dynamic_total, window=5)
indiv.ma <- computeMovingAverage(dynamic_indiv,dynamic_total, window=5)
```

New size:

```{r nrow2}
nrow(relations.ma)
```


## Caculating a set of measures by year

This step calculates the measures used as a basis for calculating the trend (next step). One or several measures can be calculated.

Available measures:

* `prob.joint` is the simple joint probability (it is already calculated from the previous step but this step is still recommended for consistency).
* `pmi` and `npmi`: [Pointwise Mutual Information](https://en.wikipedia.org/wiki/Pointwise_mutual_information) and its normalized variant.
* `mi` and `nmi`: "binary" [Mutual Information](https://en.wikipedia.org/wiki/Mutual_information) and its normalized variant. The events considered are simply based on whether each concept is present or not (hence the word "binary").
* `scp`
* `pmi2` and `pmi3`

```{r measures}
rel.measures<-addDynamicAssociationToRelations(relations.ma,indiv.ma,measures = c('prob.joint','pmi','nmi'))
```

## Calculating the trend for every year and every relation

This step calculates the `trend` for every year and every relation. 

- The input data table is modified in place for the sake of efficiency. It contains additional columns after executing the function, but the number of rows is not modified. 
- The trend is calculated using the column provided with the argument `measure` and stored in a new column `trend`.
- The `indicator` argument determines how the `trend` value is calculated:
    - `rate` (default) is the relative rate: $\frac{p_{y}-p_{y-1}}{p_{y-1}}$. Experimetal results show that this indicator gives very poor results.
    - `diff` is the simple difference: $p_{y}-p_{y-1}$

Example:

```{r trend}
computeTrend(rel.measures, indicator='diff', measure='nmi')
```

- Note: there is no need to store the output data table since the data table is modified by reference.

## Detecting surges

For every relation, this step marks the years where the trend value is higher than some threshold $t$ as surge. 

$t$ can be a custom threshold or defined with one of the two proposed methods:

- method 1 uses the standard outlier threshold calculated with the inter-quartile range: $t=Q_3 + 3 IQR$.
- method 2 (recommended) uses the inflection point in the quantile graph (see details in the paper).

The input data table is modified in place for the sake of efficiency. 


```{r surges}
# threshold <- calculateThresholdTopOutliers(rel.measures$trend)
threshold <- calculateThresholdInflectionPoint(rel.measures$trend)
print(threshold)
detectSurges(rel.measures, globalThreshold=threshold)
```


## Adjusting the surge year in the sliding window (optional)

Using a moving average window (size higher than 1) can cause the surge year to be detected too early. This can lead to a meaningless result, in particular if the surge year has no cooccurrence at all. This step calculates the next non-zero year for every year and every relation. This "adjusted year" can be used to replace the surge year in some applications. 

This step is optional and takes a fairly long time (5 to 10 minutes for the ND subset).

```{r adjust}
addNextNonZeroYear(rel.measures)
```


## Statistics surges


```{r}
surges_stats <- countSurgesByRelation(rel.measures)
kable(surges_stats[n.surges<=10,])
```


```{r}
ggplot(surges_stats,aes(n.surges,prop))+geom_col()
```


## Processing multiple parameters and saving surges data


A convenience function is provided which computes the surges based on multiple parameters and saves the resulting data to a file. This function encapsulates all the steps presented above.

```{r saving,eval=!shortVersion}
system.time(computeAndSaveSurgesData(dataPath,outputDir=outputDir,suffix=data.suffix, ma_windows=c(1,3,5),measures=c('prob.joint','pmi','npmi','mi','nmi','scp'),indicators=c('rate','diff')))
```
